{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXEG1umFh-aY"
      },
      "source": [
        "<img src=\"https://github.com/UBC-NLP/afrolid/raw/main/images/afrolid_logo.jpg\">\n",
        "\n",
        "AfroLID, a neural LID toolkit for 517 African languages and varieties. AfroLID exploits a multi-domain web dataset manually curated from across 14 language families utilizing five orthographic systems. AfroLID is described in this paper:\n",
        "[**AfroLID: A Neural Language Identification Tool for African Languages**](https://arxiv.org/abs/2210.11744).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4reRbEghiBxt"
      },
      "source": [
        "## (1) Install Afrolid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pip==24.0"
      ],
      "metadata": {
        "id": "J-9sbQNGdEej",
        "outputId": "207ca4db-0175-48cd-9c55-10b1977eeca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip==24.0 in /usr/local/lib/python3.10/dist-packages (24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.11.0 sentencepiece==0.1.96 fairseq==0.12.2 tqdm==4.63.0"
      ],
      "metadata": {
        "id": "mgup-6mliUWl",
        "outputId": "6b2c859f-1478-4ecb-8c6c-66e0c8f513e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.11.0\n",
            "  Using cached torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting sentencepiece==0.1.96\n",
            "  Using cached sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting fairseq==0.12.2\n",
            "  Using cached fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm==4.63.0\n",
            "  Using cached tqdm-4.63.0-py2.py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.11)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Using cached hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.26.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2024.9.11)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==0.12.2)\n",
            "  Using cached sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Using cached bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.5.1+cu121)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Using cached antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.2)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Using cached portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (5.3.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio>=0.8.0 (from fairseq==0.12.2)\n",
            "  Using cached torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "INFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached torchaudio-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.7 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached torchaudio-2.0.2-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
            "  Using cached torchaudio-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
            "  Using cached torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
            "  Using cached torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
            "  Using cached torchaudio-0.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
            "  Using cached torchaudio-0.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
            "  Using cached torchaudio-0.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.22)\n",
            "Using cached torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "Using cached sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
            "Using cached hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "Using cached omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Using cached sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "Using cached torchaudio-0.11.0-cp310-cp310-manylinux1_x86_64.whl (2.9 MB)\n",
            "Using cached bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Using cached portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11288618 sha256=f1c042d79825b467762f1000d04539b1cbdcfa722f7ee8664e87d54e1665921b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=c16b4e42a6feeb92d34ed7f145bb21a2701b40d83f2aeaff417a72e966e17fae\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: sentencepiece, bitarray, antlr4-python3-runtime, tqdm, torch, portalocker, omegaconf, colorama, torchaudio, sacrebleu, hydra-core, fairseq\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.2.0\n",
            "    Uninstalling sentencepiece-0.2.0:\n",
            "      Successfully uninstalled sentencepiece-0.2.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.6\n",
            "    Uninstalling tqdm-4.66.6:\n",
            "      Successfully uninstalled tqdm-4.66.6\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu121\n",
            "    Uninstalling torchaudio-2.5.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires tqdm>=4.64.1, but you have tqdm 4.63.0 which is incompatible.\n",
            "peft 0.13.2 requires torch>=1.13.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.8 bitarray-3.0.0 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-3.0.0 sacrebleu-2.4.3 sentencepiece-0.1.96 torch-1.11.0 torchaudio-0.11.0 tqdm-4.63.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyUFV9b6FaJI",
        "outputId": "adf35945-339e-4ad3-e317-6d2f5c50b47a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for afrolid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.19.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.71.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-iam 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.15.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U git+https://github.com/UBC-NLP/afrolid.git --q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch -U"
      ],
      "metadata": {
        "id": "BMQjlRrbdMI1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "golxJtBRHSvG"
      },
      "source": [
        "## (2) Donwload the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJvlcwqvHSXp",
        "outputId": "daf95755-1085-40f8-919e-9a4041faa2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-04 12:03:07--  https://demos.dlnlp.ai/afrolid/afrolid_model.tar.gz\n",
            "Resolving demos.dlnlp.ai (demos.dlnlp.ai)... 74.208.236.113, 2607:f1c0:100f:f000::264\n",
            "Connecting to demos.dlnlp.ai (demos.dlnlp.ai)|74.208.236.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2277022086 (2.1G) [application/gzip]\n",
            "Saving to: ‘afrolid_model.tar.gz’\n",
            "\n",
            "afrolid_model.tar.g 100%[===================>]   2.12G  86.8MB/s    in 31s     \n",
            "\n",
            "2024-12-04 12:03:38 (70.0 MB/s) - ‘afrolid_model.tar.gz’ saved [2277022086/2277022086]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://demos.dlnlp.ai/afrolid/afrolid_model.tar.gz\n",
        "!tar -xf afrolid_model.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXqFDD05iL21"
      },
      "source": [
        "## (2) Initial AfroLID object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ved3JKTrFfWl"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import logging\n",
        "from afrolid.main import classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xtKaQb5PYom3"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n",
        "    force=True, # Resets any previous configuration\n",
        ")\n",
        "logger = logging.getLogger(\"afrolid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP1miA5bQoqQ",
        "outputId": "97e9da14-94e8-4ac3-ec91-951b9254aa34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-12-04 12:04:32 | INFO | afrolid | Initalizing AfroLID's task and model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| [input] dictionary: 64001 types\n",
            "| [label] dictionary: 528 types\n"
          ]
        }
      ],
      "source": [
        "cl = classifier(logger, model_path=\"/content/afrolid_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bO-8_10ykkOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34U8gcSPnivj"
      },
      "source": [
        "## (3) Get language prediction(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldRl0S0KQbZz",
        "outputId": "20a22264-420a-41d5-951d-70433b9b31c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-12-04 12:04:40 | INFO | afrolid | Input text: 6Acï looi aya në wuöt dït kɔ̈k yiic ku lɔ wuöt tɔ̈u tëmec piny de Manatha ku Eparaim ku Thimion , ku ɣään mec tɔ̈u të lɔ rut cï Naptali\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted languages:\n",
            "     |-- ISO: dip\tName: Dinka, Northeastern\tScript: Latin\tScore: 100.0%\n"
          ]
        }
      ],
      "source": [
        "## Gold label = dip\n",
        "text=\"6Acï looi aya në wuöt dït kɔ̈k yiic ku lɔ wuöt tɔ̈u tëmec piny de Manatha ku Eparaim ku Thimion , ku ɣään mec tɔ̈u të lɔ rut cï Naptali\"\n",
        "predicted_langs = cl.classify(text) # default max_outputs=3\n",
        "print(\"Predicted languages:\")\n",
        "for lang in predicted_langs:\n",
        "  print(\"     |-- ISO: {}\\tName: {}\\tScript: {}\\tScore: {}%\".format(\n",
        "                      lang,\n",
        "                      predicted_langs[lang]['name'],\n",
        "                      predicted_langs[lang]['script'],\n",
        "                      predicted_langs[lang]['score']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fd7JEi5mfap",
        "outputId": "3d2c8784-13d7-43dc-d729-be29245f1c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-12-04 12:04:41 | INFO | afrolid | Input text: Ama vuodieke nɩŋ mana n Chʋa Ŋmɩŋ dɩ nagɩna yɩ mɩŋ , nan keŋ n jigiŋ a yi mɩŋ yada , ta n kaaŋ yagɩ vuodieke nɩŋ dɩ kienene n jigiŋ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted languages:\n",
            "     |-- ISO: kma\tName: Konni\tScript: Latin\tScore: 68.42%\n",
            "     |-- ISO: kmy\tName: Koma\tScript: Latin\tScore: 31.58%\n"
          ]
        }
      ],
      "source": [
        "## Gold label = kmy\n",
        "text=\"Ama vuodieke nɩŋ mana n Chʋa Ŋmɩŋ dɩ nagɩna yɩ mɩŋ , nan keŋ n jigiŋ a yi mɩŋ yada , ta n kaaŋ yagɩ vuodieke nɩŋ dɩ kienene n jigiŋ\"\n",
        "predicted_langs = cl.classify(text)  # default max_outputs=3\n",
        "print(\"Predicted languages:\")\n",
        "for lang in predicted_langs:\n",
        "  print(\"     |-- ISO: {}\\tName: {}\\tScript: {}\\tScore: {}%\".format(\n",
        "                      lang,\n",
        "                      predicted_langs[lang]['name'],\n",
        "                      predicted_langs[lang]['script'],\n",
        "                      predicted_langs[lang]['score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiDAIcmkkEnx"
      },
      "source": [
        "## (3) Integrate with Pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a58-T-g0e7yT"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/UBC-NLP/afrolid/main/examples/examples.tsv -O examples.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwmTQwGUk_Nn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "df = pd.read_csv(\"examples.tsv\", sep=\"\\t\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC2ycVKAEsmA"
      },
      "outputs": [],
      "source": [
        "def get_afrolid_prediction(text):\n",
        "  predictions = cl.classify(text, max_outputs=1)\n",
        "  for lang in predictions:\n",
        "    return lang, predictions[lang]['score'], predictions[lang]['name'], predictions[lang]['script']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pxAlVkrlFbT"
      },
      "outputs": [],
      "source": [
        "df['predict_iso'], df['predict_score'], df['predict_name'], df['predict_script'] = zip(*df['content'].progress_apply(get_afrolid_prediction))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZks8EnElsFj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "543df3536ce0562e803940352a53947075ebc9a19d53a312c04f27cec071e210"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}